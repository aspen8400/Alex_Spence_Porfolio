{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "# Data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Text Processing\n",
    "from sklearn.feature_extraction import text as tx\n",
    "from scipy import sparse\n",
    "from nltk.corpus import words as nltk_words\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from langdetect import detect\n",
    "import enchant\n",
    "\n",
    "# Plotting\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import roc_auc_score as auc, average_precision_score as ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "pio.renderers.default = \"browser\"\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Use Enchant English Dictionary\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(train_name, val_name, df_trn, model, label):\n",
    "    # Upload Training Data\n",
    "    y_trn = df_trn['label']\n",
    "    X_trn = sparse.load_npz(os.path.join(\"data\", train_name))\n",
    "    X_trn = sparse.hstack((X_trn, df_trn.drop(columns=['label']).values))\n",
    "    \n",
    "    # Upload Validation Data\n",
    "    y_val = df_val['label']\n",
    "    X_val = sparse.load_npz(os.path.join(\"data\", val_name))\n",
    "    X_val = sparse.hstack((X_val, df_val.drop(columns=['label']).values))    \n",
    "    \n",
    "    # Train Model\n",
    "    model = model.fit(X_trn, y_trn)\n",
    "    yhat_trn = model.predict_proba(X_trn)\n",
    "    #yhat_trn = model.predict(X_trn)\n",
    "    yhat_val = model.predict_proba(X_val)\n",
    "    \n",
    "    # Results\n",
    "    auc_trn = auc(y_trn, yhat_trn[:, 1])\n",
    "    auc_val = auc(y_val, yhat_val[:, 1])\n",
    "    ap_trn = ap(y_trn, yhat_trn[:, 1])\n",
    "    ap_val = ap(y_val, yhat_val[:, 1])\n",
    "    \n",
    "    return {'model':label, 'data':val_name, 'auc':auc_val, \n",
    "              'ap':ap_val, 'auc_trn':auc_trn, 'ap_trn':ap_trn}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(os.path.join('data', 'df_train.csv'), index_col=['ex_id'])\n",
    "df_val = pd.read_csv(os.path.join('data', 'df_valid.csv'), index_col=['ex_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [['wordcount_train.npz', 'wordcount_valid.npz'],\n",
    "            ['tfidfnorm_trim_train.npz', 'tfidfnorm_trim_valid.npz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in datasets:\n",
    "    results.append(train_val(data[0], data[1], df_trn, BernoulliNB(), 'Naive Bayes (default)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up-Sampled Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(os.path.join('data', 'df_train_up.csv'), index_col=['ex_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [['wordcount_train_up.npz', 'wordcount_valid_up.npz'],\n",
    "            ['tfidfnorm_trim_train_up.npz', 'tfidfnorm_trim_valid_up.npz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in datasets:\n",
    "    results.append(train_val(data[0], data[1], df_trn, BernoulliNB(), 'Naive Bayes (default)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(os.path.join('data', 'df_train.csv'), index_col=['ex_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [['ngram_lem_train.npz', 'ngram_lem_valid.npz'],\n",
    "            ['ngram_snow_tfidf_train.npz', 'ngram_snow_tfidf_valid.npz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in datasets:\n",
    "    results.append(train_val(data[0], data[1], df_trn, BernoulliNB(), 'Naive Bayes (default)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   model    6 non-null      object \n",
      " 1   data     6 non-null      object \n",
      " 2   auc      6 non-null      float64\n",
      " 3   ap       6 non-null      float64\n",
      " 4   auc_trn  6 non-null      float64\n",
      " 5   ap_trn   6 non-null      float64\n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 416.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data</th>\n",
       "      <th>auc</th>\n",
       "      <th>ap</th>\n",
       "      <th>auc_trn</th>\n",
       "      <th>ap_trn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes (default)</td>\n",
       "      <td>ngram_snow_tfidf_valid.npz</td>\n",
       "      <td>0.703675</td>\n",
       "      <td>0.208643</td>\n",
       "      <td>0.887085</td>\n",
       "      <td>0.474335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes (default)</td>\n",
       "      <td>ngram_lem_valid.npz</td>\n",
       "      <td>0.705168</td>\n",
       "      <td>0.207704</td>\n",
       "      <td>0.878633</td>\n",
       "      <td>0.449419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes (default)</td>\n",
       "      <td>tfidfnorm_trim_valid_up.npz</td>\n",
       "      <td>0.675065</td>\n",
       "      <td>0.195291</td>\n",
       "      <td>0.728830</td>\n",
       "      <td>0.714939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (default)</td>\n",
       "      <td>tfidfnorm_trim_valid.npz</td>\n",
       "      <td>0.678018</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.704599</td>\n",
       "      <td>0.224764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes (default)</td>\n",
       "      <td>wordcount_valid.npz</td>\n",
       "      <td>0.666764</td>\n",
       "      <td>0.186743</td>\n",
       "      <td>0.693051</td>\n",
       "      <td>0.211223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes (default)</td>\n",
       "      <td>wordcount_valid_up.npz</td>\n",
       "      <td>0.665040</td>\n",
       "      <td>0.185383</td>\n",
       "      <td>0.706379</td>\n",
       "      <td>0.686171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model                         data       auc        ap  \\\n",
       "5  Naive Bayes (default)   ngram_snow_tfidf_valid.npz  0.703675  0.208643   \n",
       "4  Naive Bayes (default)          ngram_lem_valid.npz  0.705168  0.207704   \n",
       "3  Naive Bayes (default)  tfidfnorm_trim_valid_up.npz  0.675065  0.195291   \n",
       "1  Naive Bayes (default)     tfidfnorm_trim_valid.npz  0.678018  0.195122   \n",
       "0  Naive Bayes (default)          wordcount_valid.npz  0.666764  0.186743   \n",
       "2  Naive Bayes (default)       wordcount_valid_up.npz  0.665040  0.185383   \n",
       "\n",
       "    auc_trn    ap_trn  \n",
       "5  0.887085  0.474335  \n",
       "4  0.878633  0.449419  \n",
       "3  0.728830  0.714939  \n",
       "1  0.704599  0.224764  \n",
       "0  0.693051  0.211223  \n",
       "2  0.706379  0.686171  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(['ap', 'auc'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DEFAULT]",
   "language": "python",
   "name": "conda-env-DEFAULT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
