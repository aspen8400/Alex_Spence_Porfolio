{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "In this notebook, we generate features by tokenizing and transforming the review text. All other features are created in the \"Additional Features\" Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Packages for Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Packages for Text Processing\n",
    "from sklearn.feature_extraction import text as tx\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "import enchant\n",
    "import spacy\n",
    "\n",
    "# Plotting\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Use Enchant English Dictionary\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "# Load Spacy English Language Model (Note, you need to install Spacy models in addition to the package)\n",
    "# sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data Import\n",
    "df_trn = pd.read_csv(os.path.join('..', 'data', 'train_raw.csv'), parse_dates=['date'], index_col=['ex_id'])\n",
    "df_val = pd.read_csv(os.path.join('..', 'data', 'dev_raw.csv'), parse_dates=['date'], index_col=['ex_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.004956\n"
     ]
    }
   ],
   "source": [
    "# Train Count Vectorizer\n",
    "start = pd.Timestamp.now()\n",
    "count_trans = tx.CountVectorizer(strip_accents='unicode').fit(df_trn['review'])\n",
    "end = pd.Timestamp.now() \n",
    "print((end - start)/pd.Timedelta('1s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.271968\n"
     ]
    }
   ],
   "source": [
    "start = pd.Timestamp.now()\n",
    "X_count_trn = count_trans.transform(df_trn['review'])\n",
    "X_count_val = count_trans.transform(df_val['review'])\n",
    "end = pd.Timestamp.now() \n",
    "print((end - start)/pd.Timedelta('1s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer + Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tfconnor\\Anaconda3\\envs\\DEFAULT\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning:\n",
      "\n",
      "The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.341669766666666\n"
     ]
    }
   ],
   "source": [
    "start = pd.Timestamp.now()\n",
    "count_lem_trans = tx.CountVectorizer(tokenizer=LemmaTokenizer(), strip_accents='unicode').fit(df_trn['review'])\n",
    "end = pd.Timestamp.now() \n",
    "print((end - start)/pd.Timedelta('1m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.6067006\n"
     ]
    }
   ],
   "source": [
    "start = pd.Timestamp.now()\n",
    "X_count_lem_trn = count_lem_trans.transform(df_trn['review'])\n",
    "X_count_lem_val = count_lem_trans.transform(df_val['review'])\n",
    "end = pd.Timestamp.now() \n",
    "print((end - start)/pd.Timedelta('1m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250874, 182746)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_count_lem_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('wordcount_train', X_count_lem_trn)\n",
    "sparse.save_npz('wordcount_valid', X_count_lem_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to TFIDF and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_trans = tx.TfidfTransformer().fit(X_count_lem_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_trn = tfidf_trans.transform(X_count_lem_trn)\n",
    "X_tfidf_val = tfidf_trans.transform(X_count_lem_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('tfidfnorm_train', X_tfidf_trn)\n",
    "sparse.save_npz('tfidfnorm_valid', X_tfidf_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_trans = tx.TfidfTransformer(norm=None).fit(X_count_lem_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_trn = tfidf_trans.transform(X_count_lem_trn)\n",
    "X_tfidf_val = tfidf_trans.transform(X_count_lem_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('tfidfraw_train', X_count_lem_trn)\n",
    "sparse.save_npz('tfidfraw_valid', X_count_lem_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + Lemmatizer + Remove words w/ 2 or fewer occurences + Remove English Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First remove infrequent words from the CountVectorizer that we trained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = X_count_lem_trn.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        110049\n",
       "2         20169\n",
       "3          9176\n",
       "4          5599\n",
       "5          3796\n",
       "          ...  \n",
       "4168          1\n",
       "30917         1\n",
       "12492         1\n",
       "4552          1\n",
       "10245         1\n",
       "Name: 0, Length: 2628, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_count.T).iloc[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words with 1 or 2\n",
    "s_keep = pd.DataFrame(word_count.T, index=count_lem_trans.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182746"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_keep = s_keep[s_keep>=2].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72697"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Remove English Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_keep_idx = [x for x in s_keep.index if x not in set(stopwords.words('english'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998046686933436"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_keep_idx)/len(s_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_keep = s_keep.loc[s_keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72555"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_idx = [count_lem_trans.vocabulary_[x] for x in s_keep.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = X_count_lem_trn[:, keep_idx]\n",
    "X_val = X_count_lem_val[:, keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Word Count to TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_trans = tx.TfidfTransformer().fit(X_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_trn = tfidf_trans.transform(X_trn)\n",
    "X_tfidf_val = tfidf_trans.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('tfidfnorm_trim_train', X_tfidf_trn)\n",
    "sparse.save_npz('tfidfnorm_trim_valid', X_tfidf_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_trans = tx.TfidfTransformer(norm=None).fit(X_trn)\n",
    "X_tfidf_trn = tfidf_trans.transform(X_trn)\n",
    "X_tfidf_val = tfidf_trans.transform(X_val)\n",
    "sparse.save_npz('tfidfraw_trim_train', X_tfidf_trn)\n",
    "sparse.save_npz('tfidfraw_trim_valid', X_tfidf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DEFAULT]",
   "language": "python",
   "name": "conda-env-DEFAULT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
